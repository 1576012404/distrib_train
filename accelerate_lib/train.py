import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from accelerate import Accelerator
import time


# --- 1. å®šä¹‰ä¸€ä¸ªç®€å•çš„åˆæˆæ•°æ®é›† ---
# ç›®çš„ï¼šæ’é™¤ç¡¬ç›˜ IO ç“¶é¢ˆï¼Œçº¯æµ‹ GPU é€Ÿåº¦
class SyntheticDataset(Dataset):
    def __init__(self, size=10000, img_shape=(3, 224, 224)):
        self.size = size
        self.img_shape = img_shape
        # é¢„å…ˆç”Ÿæˆæ•°æ®æ”¾åœ¨å†…å­˜é‡Œ
        self.data = torch.randn(size, *img_shape)
        self.target = torch.randint(0, 1000, (size,))

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        return self.data[idx], self.target[idx]


def main():
    # --- 2. åˆå§‹åŒ– Accelerator ---
    # è¿™é‡Œä¼šè‡ªåŠ¨æ£€æµ‹å‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯ç”¨ fp16 è¿˜æ˜¯ fp32ï¼Œæ˜¯ç”¨ 1 å¼ å¡è¿˜æ˜¯ 2 å¼ å¡
    accelerator = Accelerator()

    # æ‰“å°å½“å‰çš„è¿è¡ŒçŠ¶æ€
    accelerator.print(f"ğŸš€ å¯åŠ¨é…ç½®: è¿›ç¨‹æ•°={accelerator.num_processes}, "
                      f"ç²¾åº¦={accelerator.mixed_precision}, "
                      f"è®¾å¤‡={accelerator.device}")

    # --- 3. å‡†å¤‡æ¨¡å‹å’Œæ•°æ® ---
    # ä½¿ç”¨ ResNet50 (ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œåªæµ‹è®¡ç®—é‡)
    from torchvision.models import resnet50
    model = resnet50(num_classes=1000)

    # æ³¨æ„ï¼šBatch Size ä¹Ÿæ˜¯éšç€å¡æ•°çº¿æ€§å¢åŠ çš„
    # å•å¡ batch=64ï¼ŒåŒå¡å®é™…ä¸Š global_batch=128
    batch_size = 64
    dataset = SyntheticDataset(size=2000)  # è·‘ 2000 ä¸ªæ ·æœ¬åšæµ‹è¯•
    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2, pin_memory=True)

    optimizer = optim.SGD(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()

    # --- 4. Prepare (Accelerate é­”æ³•æ—¶åˆ») ---
    model, optimizer, dataloader = accelerator.prepare(
        model, optimizer, dataloader
    )

    # --- 5. è®­ç»ƒå¾ªç¯ä¸è®¡æ—¶ ---
    model.train()

    # é¢„çƒ­ (Warmup)ï¼šè·‘å‡ æ­¥è®© CUDA kernel åˆå§‹åŒ–ï¼Œé¿å…å½±å“è®¡æ—¶
    accelerator.print("ğŸ”¥ å¼€å§‹é¢„çƒ­...")
    for i, batch in enumerate(dataloader):
        if i > 5: break
        inputs, targets = batch
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        accelerator.backward(loss)
        optimizer.step()
        optimizer.zero_grad()

    accelerator.wait_for_everyone()  # ç­‰å¾…æ‰€æœ‰å¡é¢„çƒ­å®Œæ¯•

    # æ­£å¼è®¡æ—¶
    accelerator.print("â±ï¸  å¼€å§‹æ­£å¼æµ‹è¯•...")
    start_time = time.time()
    total_samples = 0

    for inputs, targets in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        accelerator.backward(loss)
        optimizer.step()

        # ç»Ÿè®¡æ ·æœ¬æ•° (æ³¨æ„ï¼šå¦‚æœæ˜¯å¤šå¡ï¼Œæ¯å¼ å¡å¤„ç† batch_size ä¸ª)
        total_samples += inputs.size(0)

    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹è·‘å®Œ
    accelerator.wait_for_everyone()
    end_time = time.time()

    # --- 6. ç»“æœæ±‡æ€» ---
    # åªåœ¨ä¸»è¿›ç¨‹è®¡ç®—æ€»ååé‡
    if accelerator.is_main_process:
        # åœ¨å¤šå¡æ¨¡å¼ä¸‹ï¼Œtotal_samples åªæ˜¯ä¸»è¿›ç¨‹çœ‹åˆ°çš„æ•°é‡
        # æ€»å¤„ç†é‡ = å•è¿›ç¨‹å¤„ç†é‡ * è¿›ç¨‹æ•° (å‡è®¾æ•°æ®åˆ†é…å‡åŒ€)
        # æˆ–è€…æ›´ä¸¥è°¨çš„åšæ³•æ˜¯ gather æ‰€æœ‰å¡çš„æ•°æ®ï¼Œä½†è¿™é‡Œåšè¿‘ä¼¼ä¼°ç®—å³å¯
        total_processed = total_samples * accelerator.num_processes
        duration = end_time - start_time
        throughput = total_processed / duration

        print("-" * 40)
        print(f"âœ… æµ‹è¯•å®Œæˆï¼")
        print(f"è€—æ—¶: {duration:.2f} ç§’")
        print(f"æ€»ååé‡: {throughput:.2f} samples/sec")
        print(f"è¯´æ˜: æ•°å€¼è¶Šå¤§ï¼Œæ€§èƒ½è¶Šå¥½")
        print("-" * 40)


if __name__ == "__main__":
    main()