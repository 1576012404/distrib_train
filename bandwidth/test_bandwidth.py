import torch

def test_p2p_bandwidth():
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ GPU
    if torch.cuda.device_count() < 2:
        print("å½“å‰ç¯å¢ƒåªæœ‰å•å¡æˆ–æ—  GPUï¼Œéœ€è¦è‡³å°‘ä¸¤å¼ æ˜¾å¡æ‰èƒ½æµ‹è¯•ï¼")
        return

    print(f"æ£€æµ‹åˆ° {torch.cuda.device_count()} å¼ æ˜¾å¡ã€‚æ­£åœ¨å‡†å¤‡æµ‹è¯•...")
    device0 = torch.device('cuda:0')
    device1 = torch.device('cuda:1')

    # å‡†å¤‡ 1GB çš„æ•°æ® (256M ä¸ª float32 å…ƒç´  = 1024 MB)
    size_in_bytes = 1024 * 1024 * 1024
    # ä½¿ç”¨ empty å¯ä»¥ç¬é—´åˆ†é…æ˜¾å­˜è€Œä¸è¿›è¡Œå¤šä½™è®¡ç®—
    tensor0 = torch.empty(256 * 1024 * 1024, dtype=torch.float32, device=device0)

    # 1. é¢„çƒ­ (Warmup) - è®© GPU é¢„å…ˆåˆ†é…å¥½åº•å±‚èµ„æºï¼Œé¿å…å†·å¯åŠ¨è¯¯å·®
    for _ in range(5):
        tensor1 = tensor0.to(device1)
    torch.cuda.synchronize()

    # 2. å¼€å§‹æµ‹é€Ÿ
    iterations = 20
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)

    # è®°å½•èµ·ç‚¹
    start_event.record()
    for _ in range(iterations):
        tensor1 = tensor0.to(device1)
    # è®°å½•ç»ˆç‚¹
    end_event.record()

    # ç­‰å¾…æ‰€æœ‰ CUDA ä»»åŠ¡å®Œæˆ
    torch.cuda.synchronize()

    # 3. è®¡ç®—æ—¶é—´ä¸å¸¦å®½
    elapsed_time_ms = start_event.elapsed_time(end_event)
    elapsed_time_s = elapsed_time_ms / 1000.0

    total_transferred_gb = (size_in_bytes * iterations) / (1024**3)
    bandwidth = total_transferred_gb / elapsed_time_s

    print("-" * 40)
    print(f"ä¼ è¾“æ€»æ•°æ®é‡: {total_transferred_gb} GB")
    print(f"æ€»è€—æ—¶: {elapsed_time_s:.4f} ç§’")
    print(f"ğŸ‘‰ GPU 0 åˆ° GPU 1 çš„å®é™…å•å‘å¸¦å®½ä¼°ç®—: **{bandwidth:.2f} GB/s**")
    print("-" * 40)

test_p2p_bandwidth()